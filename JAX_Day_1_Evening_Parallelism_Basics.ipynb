{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX Day 1 Evening - Parallelism Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJWwh43TTSCIQR+ViyLW4E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halcy/LearningJAX/blob/main/JAX_Day_1_Evening_Parallelism_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZsydUDmQuBT"
      },
      "source": [
        "# JAX Day 1 - Evening - Parallelism\n",
        "\n",
        "Now that we can train an MNIST digits classifier, lets take a step back and see how to do some basic parallelism so we can go faster!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI8gUnVLwrlV",
        "outputId": "93596494-c6d5-49ea-cb4c-59faac41e262"
      },
      "source": [
        "# Catchall \"what is this runtime\" cell\n",
        "!nvidia-smi\n",
        "GPU = !nvidia_smi\n",
        "\n",
        "if len(GPU) > 3:\n",
        "    GPU = True\n",
        "else:\n",
        "    GPU = False\n",
        "\n",
        "!vmstat\n",
        "print(\"\")\n",
        "\n",
        "import os\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    from tensorflow.python.profiler import profiler_client\n",
        "    print(\"tpu:\", os.environ['COLAB_TPU_ADDR'])\n",
        "    tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
        "    print(profiler_client.monitor(tpu_profile_service_address, 100, 2).strip())\n",
        "    TPU = True\n",
        "else:\n",
        "    print(\"tpu: no\")\n",
        "    TPU = False\n",
        "\n",
        "CPUS = os.cpu_count()\n",
        "print(\"\\ncpus:\", CPUS)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n",
            " r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n",
            " 1  0      0 10670764 101884 2033540    0    0  7450   172  908 1809 15  7 69  9  0\n",
            "\n",
            "tpu: 10.122.222.106:8470\n",
            "Timestamp: 06:27:59\n",
            "  TPU type: TPU v2\n",
            "  Utilization of TPU Matrix Units (higher is better): 0.000%\n",
            "\n",
            "cpus: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcWWDPaGQmTG"
      },
      "source": [
        "# Basic setup and data loading\n",
        "\n",
        "Basically the same as the previous notebook, but now we also want xmap and mesh from jax.experimental.maps, to parallelize computations across multiple TPU devices, and we don't actually need MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmhGBH-7zUJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229470ac-ae23-41b8-ab22-a2054c8d3a9e"
      },
      "source": [
        "# Set JAX, haiku and optax up for the TPU\n",
        "!pip install --upgrade -q jax jaxlib dm-haiku optax tqdm\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "    url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "    resp = requests.post(url)\n",
        "    TPU_DRIVER_MODE = 1\n",
        "\n",
        "# TPU driver as backend for JAX\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 708 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 284 kB 44.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.8 MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHfiPH6dvtTQ"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, nn\n",
        "\n",
        "from jax.experimental.maps import xmap, mesh\n",
        "\n",
        "import haiku as hk\n",
        "import optax\n",
        "\n",
        "import tqdm\n",
        "\n",
        "# Pinky promise: We are now aware xmap is experimental, and will adjust our expectations accordingly\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"xmap is an experimental feature and probably has bugs!\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbHUX9RfxFKs"
      },
      "source": [
        "# Generate PRNG state\n",
        "prng = jax.random.PRNGKey(23)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgVg9FFcVCdV"
      },
      "source": [
        "# Basic data parallelism example\n",
        "xmap lets us run a function in parallel on all available devices with relative ease. It does this using named axes. Lets see how, and how we can use that to Go Fast!\n",
        "\n",
        "Note that we're only using named axes in a very simple manner here - it wouldn't have been hard to just use positional axes instead. However, named axes allow jax to keep track of how to split computation across more than just batches, which seems fairly powerful and might be useful later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfbFpsw7VGsv",
        "outputId": "f1d0de4e-b3f1-42d0-d072-89bd43adb31b"
      },
      "source": [
        "# A very simple feedforward network\n",
        "def simple_nn(x):\n",
        "    for i in range(100):\n",
        "        lin1 = hk.Linear(100)\n",
        "        x = lin1(x)\n",
        "    lin2 = hk.Linear(1)        \n",
        "    x = lin2(x)\n",
        "    return x\n",
        "\n",
        "# Set up the model\n",
        "in_shape = (100000, 1)\n",
        "data = np.random.normal(size=in_shape)\n",
        "\n",
        "model = hk.transform(simple_nn)\n",
        "params = model.init(prng, data)\n",
        "\n",
        "# Run the model a lot, without coming back from the TPU, using jax.laxi.fori_loop\n",
        "# Note that when not doing this (i.e. syncing relatively often between TPU and host) the regular, one device versions\n",
        "# will often be _faster_ than the multi-device version!\n",
        "def predict(x):\n",
        "    x = jax.lax.fori_loop(0, 10000, lambda _, xt: model.apply(params, None, xt), x) # Probably a good idea to do this for your training loop!\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3133: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onMqY8vVvyQz",
        "outputId": "898d387f-a875-4f5b-93c0-ead6338678d5"
      },
      "source": [
        "# Run using only one device, with jit()\n",
        "basic_jitted = jit(predict)\n",
        "\n",
        "print(\"jit(), first run:\") # Run things twice - first time includes compilation, second does not\n",
        "data = np.random.normal(size=in_shape)\n",
        "%time print(np.mean(basic_jitted(data)))\n",
        "\n",
        "print(\"\\njit(), second run:\")\n",
        "data = np.random.normal(size=in_shape)\n",
        "%time print(np.mean(basic_jitted(data)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jit(), first run:\n",
            "0.0\n",
            "CPU times: user 18.6 s, sys: 53.6 s, total: 1min 12s\n",
            "Wall time: 2min 6s\n",
            "\n",
            "jit(), second run:\n",
            "0.0\n",
            "CPU times: user 15.8 s, sys: 54.5 s, total: 1min 10s\n",
            "Wall time: 2min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf_Z2UQAv2Z7",
        "outputId": "1063bfcd-5e90-4fe4-9b70-9f09769df36b"
      },
      "source": [
        "# Now, do the same using xmap - but still only a single device \n",
        "# All we've done is name the first axis in the input \"batch\"\n",
        "in_axes = [\"batch\", ...]\n",
        "out_axes = [\"batch\", ...]\n",
        "basic_xmapped = xmap(predict, in_axes, out_axes)\n",
        "\n",
        "print(\"\\nxmap(), first run:\")\n",
        "data = np.random.normal(size=in_shape)\n",
        "%time print(np.mean(basic_xmapped(data)))\n",
        "\n",
        "print(\"\\nxmap(), second run:\")\n",
        "data = np.random.normal(size=in_shape)\n",
        "%time print(np.mean(basic_xmapped(data)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "xmap(), first run:\n",
            "0.0\n",
            "CPU times: user 18.1 s, sys: 54.9 s, total: 1min 12s\n",
            "Wall time: 2min 6s\n",
            "\n",
            "xmap(), second run:\n",
            "0.0\n",
            "CPU times: user 16 s, sys: 54.4 s, total: 1min 10s\n",
            "Wall time: 2min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SMuwHspv9JE",
        "outputId": "8d04ab10-3da7-41a7-d360-8fbd5dbcd4bf"
      },
      "source": [
        "# Now, lets use xmap but also have it run our batch in parallel on multiple TPU devices (cores)\n",
        "# Note that for this, the axis length must be evenly divisible by the device count!\n",
        "# Things are now fast!\n",
        "parallel_xmapped = xmap(predict, in_axes, out_axes, axis_resources={'batch': 'batch_tpus'})\n",
        "with mesh(jax.devices(), ('batch_tpus',)):\n",
        "    print(\"\\nxmap(), parallel, first run:\")\n",
        "    data = np.random.normal(size=in_shape)\n",
        "    %time print(np.mean(parallel_xmapped(data)))\n",
        "\n",
        "    print(\"\\nxmap(), parallel, second run:\")\n",
        "    data = np.random.normal(size=in_shape)\n",
        "    %time print(np.mean(parallel_xmapped(data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "xmap(), parallel, first run:\n",
            "0.0\n",
            "CPU times: user 4.14 s, sys: 9.01 s, total: 13.1 s\n",
            "Wall time: 21.5 s\n",
            "\n",
            "xmap(), parallel, second run:\n",
            "0.0\n",
            "CPU times: user 2.64 s, sys: 8.58 s, total: 11.2 s\n",
            "Wall time: 19.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI215Of8RCxh"
      },
      "source": [
        "# Next up:\n",
        "\n",
        "Tomorrow, we convert the MNIST network to train in parallel!"
      ]
    }
  ]
}