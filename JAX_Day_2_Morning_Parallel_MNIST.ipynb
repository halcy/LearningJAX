{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX Day 2 Morning - Parallel MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNxrsEewhlodaYQtzgJsB+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halcy/LearningJAX/blob/main/JAX_Day_2_Morning_Parallel_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZsydUDmQuBT"
      },
      "source": [
        "# JAX Day 2 - Morning - MNIST, but fast\n",
        "\n",
        "Lets combine our knowledge of how to go wide with the MNIST classifier and train one real fast like. (*)\n",
        "\n",
        "*: The model is honestly quite small, so if the speedup to be expected is minimal, if even present at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI8gUnVLwrlV",
        "outputId": "45a58232-88a8-4665-f66e-144abbb806a6"
      },
      "source": [
        "# Catchall \"what is this runtime\" cell\n",
        "!nvidia-smi\n",
        "GPU = !nvidia_smi\n",
        "\n",
        "if len(GPU) > 3:\n",
        "    GPU = True\n",
        "else:\n",
        "    GPU = False\n",
        "\n",
        "!vmstat\n",
        "print(\"\")\n",
        "\n",
        "import os\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    from tensorflow.python.profiler import profiler_client\n",
        "    print(\"tpu:\", os.environ['COLAB_TPU_ADDR'])\n",
        "    tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
        "    print(profiler_client.monitor(tpu_profile_service_address, 100, 2).strip())\n",
        "    TPU = True\n",
        "else:\n",
        "    print(\"tpu: no\")\n",
        "    TPU = False\n",
        "\n",
        "CPUS = os.cpu_count()\n",
        "print(\"\\ncpus:\", CPUS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n",
            " r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n",
            " 0  0      0 10232360 108240 2445160    0    0   669    39 2167 8461  3  3 92  1  0\n",
            "\n",
            "tpu: 10.101.203.26:8470\n",
            "Timestamp: 11:00:48\n",
            "  TPU type: TPU v2\n",
            "  Utilization of TPU Matrix Units (higher is better): 0.000%\n",
            "\n",
            "cpus: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcWWDPaGQmTG"
      },
      "source": [
        "# Basic setup and data loading\n",
        "\n",
        "You know the deal by now, jax, haiku, optax, xmap / mesh, MNIST loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmhGBH-7zUJO"
      },
      "source": [
        "# Set JAX, haiku and optax up for the TPU\n",
        "!pip install --upgrade -q jax jaxlib dm-haiku optax tqdm\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "    url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "    resp = requests.post(url)\n",
        "    TPU_DRIVER_MODE = 1\n",
        "\n",
        "# TPU driver as backend for JAX\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHfiPH6dvtTQ"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, nn\n",
        "\n",
        "from jax.experimental.maps import xmap, mesh\n",
        "from jax.experimental.pjit import pjit, PartitionSpec\n",
        "\n",
        "import haiku as hk\n",
        "import optax\n",
        "\n",
        "import tqdm\n",
        "\n",
        "# Pinky promise: We are now aware xmap is experimental, and will adjust our expectations accordingly\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"xmap is an experimental feature and probably has bugs!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbHUX9RfxFKs"
      },
      "source": [
        "# Generate PRNG state\n",
        "prng = jax.random.PRNGKey(23)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHVJ0A4GtCNK"
      },
      "source": [
        "# Lets get MNIST\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "# Fetch full datasets for evaluation\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
        "mnist_data = tfds.as_numpy(mnist_data)\n",
        "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "num_labels = info.features['label'].num_classes\n",
        "h, w, c = info.features['image'].shape\n",
        "num_pixels = h * w * c\n",
        "\n",
        "# Full train set\n",
        "train_images, train_labels = train_data['image'], train_data['label']\n",
        "train_labels = nn.one_hot(train_labels, num_labels)\n",
        "train_images = jnp.array(train_images.astype(jnp.float32)) # One change here: We're explicitly converting to a jax array\n",
        "\n",
        "# Full test set\n",
        "test_images, test_labels = test_data['image'], test_data['label']\n",
        "test_labels = nn.one_hot(test_labels, num_labels)\n",
        "test_images = jnp.array(test_images.astype(jnp.float32)) # Same here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "wQsxY4kxtD31",
        "outputId": "d50c167b-6d15-4d7e-e801-8ff51c8e71b2"
      },
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(test_images[0,:,:,0], interpolation=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd8b840be90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgVg9FFcVCdV"
      },
      "source": [
        "# Classifier, now parallelism capable\n",
        "\n",
        "We don't have to change a whole lot about the classifier, we just need to change some operations in the network, and the loss function to use the named \"batch\" axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJMdVQjptZ42"
      },
      "source": [
        "# Lets define a network\n",
        "class ResBlock(hk.Module):\n",
        "    def __call__(self, x):\n",
        "        conv1 = hk.Conv2D(7, (3, 3))\n",
        "        conv2 = hk.Conv2D(5, (5, 5))\n",
        "        conv3 = hk.Conv2D(1, (7, 7))\n",
        "        \n",
        "        x_in = x\n",
        "        x = jax.nn.relu(conv1(x))\n",
        "        x = jax.nn.relu(conv2(x))\n",
        "        x = jax.nn.relu(conv3(x))\n",
        "        x = hk.max_pool(x, (3, 3, 1), 1, 'SAME') # Taking the batch axis out here. It was optional needed to begin with.\n",
        "        x = x - x_in\n",
        "\n",
        "        return x + w\n",
        "\n",
        "# \"idk, a resnet?\"\n",
        "def network(x, is_training=True):\n",
        "    block1 = ResBlock()\n",
        "    bn1 = hk.BatchNorm(True, True, 0.999, axis=[\"batch\", 0, 1]) # These need to explicitly be told the batch axis exists\n",
        "\n",
        "    block2 = ResBlock()\n",
        "    bn2 = hk.BatchNorm(True, True, 0.999, axis=[\"batch\", 0, 1])\n",
        "    \n",
        "    block3 = ResBlock()\n",
        "    bn3 = hk.BatchNorm(True, True, 0.999, axis=[\"batch\", 0, 1])\n",
        "    \n",
        "    project = hk.Linear(num_labels)\n",
        "    x = block1(x)\n",
        "    x = bn1(x, is_training)\n",
        "\n",
        "    x = block2(x)\n",
        "    x = bn2(x, is_training)\n",
        "\n",
        "    x = block3(x)\n",
        "    x = bn3(x, is_training)\n",
        "\n",
        "    x = x.reshape(-1, num_pixels)\n",
        "    x = project(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BaFbjuPwYoS"
      },
      "source": [
        "# Some parameters\n",
        "batch_size = 1024 * 8 # Since we're now running in parallel, lets go proper wide\n",
        "learning_rate = 0.0001\n",
        "train_epochs = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAUQtyPvwbSZ"
      },
      "source": [
        "# Construct a model and optimizer\n",
        "model = hk.transform_with_state(network)\n",
        "opt = optax.chain(\n",
        "    optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-8),\n",
        "    optax.scale(-learning_rate)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hu-OzmGwc7X"
      },
      "source": [
        "# Init model and optimizer parameters\n",
        "params, state = xmap(lambda x: model.init(prng, x), [[\"batch\", ...]], [...])(train_images[:1, :, :, :])\n",
        "opt_params = opt.init(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsDnkkX5welH"
      },
      "source": [
        "# Define a loss function\n",
        "def loss_fn(params, state, x, y):\n",
        "    x, state = model.apply(params, state, None, x, is_training=True)\n",
        "    x = nn.log_softmax(x)\n",
        "    loss = -jnp.mean(x * y) # Explicitly tell the loss function to calculate mean over batch _and_ outputs\n",
        "    #loss = loss.reshape(()) # Eliminate all axes\n",
        "    loss = jnp.mean(loss, axis=\"batch\")\n",
        "    return loss, (loss, state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlR2xV7ctc13",
        "outputId": "f39d03ff-bc1e-4173-e6d8-78f716f87fc5"
      },
      "source": [
        "# To demonstrate: We can now calculate loss over all TPUs!\n",
        "in_axes = [\n",
        "    [...],            # Axes for params: None, really\n",
        "    [...],            # Axes for state: Same\n",
        "    [\"batch\", ...],   # Axes for x: batches, rest\n",
        "    [\"batch\", ...],   # Axes for y: Same\n",
        "]\n",
        "out_axes = [...]\n",
        "\n",
        "loss_fn_xmap = xmap(loss_fn, in_axes, out_axes, axis_resources={'batch': 'batch_tpus'})\n",
        "\n",
        "with mesh(jax.devices(), ('batch_tpus',)):\n",
        "    loss, (loss2, state) = loss_fn_xmap(params, state, train_images[:16,:,:,:], train_labels[:16, :])\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray(0.26708156, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI215Of8RCxh"
      },
      "source": [
        "# Training loop\n",
        "\n",
        "Changes here: \n",
        "  * One batch is ran via xmap (in train_step)\n",
        "  * jit(xmap()) doesn't work - xmap with empty axes specifications is equivalent, and does!\n",
        "  * We use jax.lax.fori_loop to keep syncing with the host to a minimum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCz1u5JgAP3Y",
        "outputId": "47a5bb6a-a19a-4172-980c-e33a797398c5"
      },
      "source": [
        "# Train step now runs the loss function via xmap\n",
        "def train_step(params, state, opt_params, x, y):\n",
        "    grad, (loss, state) = jax.grad(loss_fn_xmap, has_aux=True)(params, state, x, y)\n",
        "    updates, opt_params = opt.update(grad, opt_params, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, state, opt_params, loss\n",
        "\n",
        "\"\"\"\n",
        "# The step is still jit-ed (but now we have to use xmap - jit() won't work!)\n",
        "train_step = xmap(train_step, [...], [...])\n",
        "\n",
        "def train_epoch(params, state, opt_params):\n",
        "    losses = []\n",
        "    for batch in jnp.arange(0, len(train_images), batch_size):\n",
        "        x = train_images[batch:batch+batch_size,:,:,:]\n",
        "        y = train_labels[batch:batch+batch_size,:]\n",
        "        params, state, opt_params, loss = train_step(params, state, opt_params, x, y)\n",
        "        losses.append(loss)\n",
        "    return params, state, opt_params, losses\n",
        "\"\"\"\n",
        "\n",
        "# Alternately, we can convert to run the training epoch with fori_loop\n",
        "def train_epoch(params, state, opt_params):\n",
        "    losses = jnp.zeros(int(len(train_images) // batch_size))\n",
        "    epoch_state = (params, state, opt_params, losses)\n",
        "    def train_step_internal(idx, epoch_state, train_images, train_labels):\n",
        "        (params, state, opt_params, losses) = epoch_state\n",
        "        batch = idx * batch_size\n",
        "        x = jax.lax.dynamic_slice(train_images, [batch, 0, 0, 0], [batch_size, 28, 28, 1])  # We have to use dynamic_slice, numpy style indexing won't work\n",
        "        y = jax.lax.dynamic_slice(train_labels, [batch, 0], [batch_size, 10])\n",
        "        params, state, opt_params, loss = train_step(params, state, opt_params, x, y)\n",
        "        loss = jnp.array(loss).reshape(1,)\n",
        "        losses = jax.lax.dynamic_update_slice(losses, loss, (idx,)) # Similarly, we have to store our losses using dynamic_update_slice\n",
        "        state_out = (params, state, opt_params, losses)\n",
        "        return state_out\n",
        "\n",
        "    params, state, opt_params, losses = jax.lax.fori_loop(0, int(len(train_images) // batch_size), lambda i, x: train_step_internal(i, x, train_images, train_labels), epoch_state)\n",
        "    return params, state, opt_params, losses\n",
        "\n",
        "# Now, jit-ing the whole epoch is sensible - note that we actually _have_ to do this, otherwise fori_loop will\n",
        "# cause our devices to disappear the same way jit() does\n",
        "train_epoch = xmap(train_epoch, [...], [...])\n",
        "\n",
        "# We have to use the context manager to make ressources available\n",
        "with mesh(jax.devices(), ('batch_tpus',)):\n",
        "    for step in range(train_epochs):\n",
        "        losses = []\n",
        "        params, state, opt_params, losses = train_epoch(params, state, opt_params)\n",
        "        print(\"e:\", step, \"l:\", np.mean(losses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e: 0 l: 0.2631526\n",
            "e: 1 l: 0.24863295\n",
            "e: 2 l: 0.23483178\n",
            "e: 3 l: 0.22201818\n",
            "e: 4 l: 0.21049435\n",
            "e: 5 l: 0.20028307\n",
            "e: 6 l: 0.19113491\n",
            "e: 7 l: 0.18286307\n",
            "e: 8 l: 0.17540398\n",
            "e: 9 l: 0.16870299\n",
            "e: 10 l: 0.16270956\n",
            "e: 11 l: 0.15743463\n",
            "e: 12 l: 0.15285432\n",
            "e: 13 l: 0.1489542\n",
            "e: 14 l: 0.14570253\n",
            "e: 15 l: 0.14314955\n",
            "e: 16 l: 0.14132613\n",
            "e: 17 l: 0.14009792\n",
            "e: 18 l: 0.139308\n",
            "e: 19 l: 0.13839132\n",
            "e: 20 l: 0.13715398\n",
            "e: 21 l: 0.13572226\n",
            "e: 22 l: 0.13436827\n",
            "e: 23 l: 0.13332886\n",
            "e: 24 l: 0.13271955\n",
            "e: 25 l: 0.13179249\n",
            "e: 26 l: 0.13086204\n",
            "e: 27 l: 0.12961625\n",
            "e: 28 l: 0.12844354\n",
            "e: 29 l: 0.12669142\n",
            "e: 30 l: 0.12510127\n",
            "e: 31 l: 0.1243893\n",
            "e: 32 l: 0.12527484\n",
            "e: 33 l: 0.1288429\n",
            "e: 34 l: 0.13228595\n",
            "e: 35 l: 0.133617\n",
            "e: 36 l: 0.14205822\n",
            "e: 37 l: 0.15140454\n",
            "e: 38 l: 0.14901148\n",
            "e: 39 l: 0.1418332\n",
            "e: 40 l: 0.1323016\n",
            "e: 41 l: 0.12208067\n",
            "e: 42 l: 0.113537036\n",
            "e: 43 l: 0.10728891\n",
            "e: 44 l: 0.10258477\n",
            "e: 45 l: 0.0987932\n",
            "e: 46 l: 0.09514723\n",
            "e: 47 l: 0.092318855\n",
            "e: 48 l: 0.08996953\n",
            "e: 49 l: 0.08771874\n",
            "e: 50 l: 0.086023144\n",
            "e: 51 l: 0.08434478\n",
            "e: 52 l: 0.08307443\n",
            "e: 53 l: 0.081502855\n",
            "e: 54 l: 0.08051863\n",
            "e: 55 l: 0.079383835\n",
            "e: 56 l: 0.078269616\n",
            "e: 57 l: 0.07811552\n",
            "e: 58 l: 0.077740364\n",
            "e: 59 l: 0.0744794\n",
            "e: 60 l: 0.075405\n",
            "e: 61 l: 0.07480922\n",
            "e: 62 l: 0.074139565\n",
            "e: 63 l: 0.0726166\n",
            "e: 64 l: 0.07404587\n",
            "e: 65 l: 0.07247203\n",
            "e: 66 l: 0.07198208\n",
            "e: 67 l: 0.072214946\n",
            "e: 68 l: 0.071842454\n",
            "e: 69 l: 0.06990625\n",
            "e: 70 l: 0.07130311\n",
            "e: 71 l: 0.06995525\n",
            "e: 72 l: 0.06953354\n",
            "e: 73 l: 0.069945395\n",
            "e: 74 l: 0.070572294\n",
            "e: 75 l: 0.06715223\n",
            "e: 76 l: 0.06919407\n",
            "e: 77 l: 0.068345286\n",
            "e: 78 l: 0.06658413\n",
            "e: 79 l: 0.06771715\n",
            "e: 80 l: 0.06664006\n",
            "e: 81 l: 0.06704524\n",
            "e: 82 l: 0.06672199\n",
            "e: 83 l: 0.065243416\n",
            "e: 84 l: 0.06582129\n",
            "e: 85 l: 0.0660949\n",
            "e: 86 l: 0.065893956\n",
            "e: 87 l: 0.06415513\n",
            "e: 88 l: 0.064453125\n",
            "e: 89 l: 0.064595506\n",
            "e: 90 l: 0.065316826\n",
            "e: 91 l: 0.06333867\n",
            "e: 92 l: 0.06364891\n",
            "e: 93 l: 0.06267968\n",
            "e: 94 l: 0.063090675\n",
            "e: 95 l: 0.062467538\n",
            "e: 96 l: 0.062483035\n",
            "e: 97 l: 0.062157292\n",
            "e: 98 l: 0.062310975\n",
            "e: 99 l: 0.061134163\n",
            "e: 100 l: 0.061578665\n",
            "e: 101 l: 0.06098616\n",
            "e: 102 l: 0.06122105\n",
            "e: 103 l: 0.06064554\n",
            "e: 104 l: 0.06112123\n",
            "e: 105 l: 0.061233312\n",
            "e: 106 l: 0.05990725\n",
            "e: 107 l: 0.060412116\n",
            "e: 108 l: 0.061475907\n",
            "e: 109 l: 0.05953894\n",
            "e: 110 l: 0.05935058\n",
            "e: 111 l: 0.059168555\n",
            "e: 112 l: 0.05880143\n",
            "e: 113 l: 0.05869769\n",
            "e: 114 l: 0.05859445\n",
            "e: 115 l: 0.058407012\n",
            "e: 116 l: 0.058212914\n",
            "e: 117 l: 0.05800462\n",
            "e: 118 l: 0.057874206\n",
            "e: 119 l: 0.057863742\n",
            "e: 120 l: 0.058020018\n",
            "e: 121 l: 0.0572546\n",
            "e: 122 l: 0.05741947\n",
            "e: 123 l: 0.057367753\n",
            "e: 124 l: 0.056883615\n",
            "e: 125 l: 0.056996595\n",
            "e: 126 l: 0.057013676\n",
            "e: 127 l: 0.05641283\n",
            "e: 128 l: 0.05642557\n",
            "e: 129 l: 0.056194346\n",
            "e: 130 l: 0.055835404\n",
            "e: 131 l: 0.055912726\n",
            "e: 132 l: 0.056592185\n",
            "e: 133 l: 0.05529653\n",
            "e: 134 l: 0.055254135\n",
            "e: 135 l: 0.055238478\n",
            "e: 136 l: 0.05485084\n",
            "e: 137 l: 0.054633778\n",
            "e: 138 l: 0.05450985\n",
            "e: 139 l: 0.054558128\n",
            "e: 140 l: 0.0546967\n",
            "e: 141 l: 0.054245252\n",
            "e: 142 l: 0.054041695\n",
            "e: 143 l: 0.05376971\n",
            "e: 144 l: 0.053839378\n",
            "e: 145 l: 0.05374136\n",
            "e: 146 l: 0.053753033\n",
            "e: 147 l: 0.05416117\n",
            "e: 148 l: 0.05328009\n",
            "e: 149 l: 0.053204082\n",
            "e: 150 l: 0.05304385\n",
            "e: 151 l: 0.05303316\n",
            "e: 152 l: 0.053136315\n",
            "e: 153 l: 0.052570898\n",
            "e: 154 l: 0.05247512\n",
            "e: 155 l: 0.052446403\n",
            "e: 156 l: 0.05229118\n",
            "e: 157 l: 0.05222449\n",
            "e: 158 l: 0.052036118\n",
            "e: 159 l: 0.051989462\n",
            "e: 160 l: 0.05176892\n",
            "e: 161 l: 0.05164868\n",
            "e: 162 l: 0.05163603\n",
            "e: 163 l: 0.051713865\n",
            "e: 164 l: 0.052084487\n",
            "e: 165 l: 0.05129217\n",
            "e: 166 l: 0.05120455\n",
            "e: 167 l: 0.05117655\n",
            "e: 168 l: 0.051031705\n",
            "e: 169 l: 0.051030952\n",
            "e: 170 l: 0.05068071\n",
            "e: 171 l: 0.050561197\n",
            "e: 172 l: 0.05048517\n",
            "e: 173 l: 0.05033606\n",
            "e: 174 l: 0.05031062\n",
            "e: 175 l: 0.05012797\n",
            "e: 176 l: 0.050046254\n",
            "e: 177 l: 0.049970277\n",
            "e: 178 l: 0.049888924\n",
            "e: 179 l: 0.050057698\n",
            "e: 180 l: 0.050290737\n",
            "e: 181 l: 0.049619157\n",
            "e: 182 l: 0.04949932\n",
            "e: 183 l: 0.049386494\n",
            "e: 184 l: 0.04927582\n",
            "e: 185 l: 0.04920318\n",
            "e: 186 l: 0.04910356\n",
            "e: 187 l: 0.04891653\n",
            "e: 188 l: 0.048862223\n",
            "e: 189 l: 0.048788767\n",
            "e: 190 l: 0.048759393\n",
            "e: 191 l: 0.048676316\n",
            "e: 192 l: 0.048632756\n",
            "e: 193 l: 0.04839577\n",
            "e: 194 l: 0.04832726\n",
            "e: 195 l: 0.04824918\n",
            "e: 196 l: 0.048316833\n",
            "e: 197 l: 0.04836536\n",
            "e: 198 l: 0.04825122\n",
            "e: 199 l: 0.048471943\n",
            "e: 200 l: 0.04915108\n",
            "e: 201 l: 0.0480034\n",
            "e: 202 l: 0.04791526\n",
            "e: 203 l: 0.047925506\n",
            "e: 204 l: 0.047597114\n",
            "e: 205 l: 0.047462396\n",
            "e: 206 l: 0.047506377\n",
            "e: 207 l: 0.0475696\n",
            "e: 208 l: 0.047523808\n",
            "e: 209 l: 0.047439832\n",
            "e: 210 l: 0.047495708\n",
            "e: 211 l: 0.04710804\n",
            "e: 212 l: 0.047043275\n",
            "e: 213 l: 0.04711006\n",
            "e: 214 l: 0.047361888\n",
            "e: 215 l: 0.047782265\n",
            "e: 216 l: 0.0468379\n",
            "e: 217 l: 0.04683795\n",
            "e: 218 l: 0.04697513\n",
            "e: 219 l: 0.04671178\n",
            "e: 220 l: 0.047119047\n",
            "e: 221 l: 0.047297075\n",
            "e: 222 l: 0.04654824\n",
            "e: 223 l: 0.046593126\n",
            "e: 224 l: 0.04685467\n",
            "e: 225 l: 0.046339646\n",
            "e: 226 l: 0.046375617\n",
            "e: 227 l: 0.04612762\n",
            "e: 228 l: 0.046015337\n",
            "e: 229 l: 0.04621198\n",
            "e: 230 l: 0.04672826\n",
            "e: 231 l: 0.0458263\n",
            "e: 232 l: 0.045811903\n",
            "e: 233 l: 0.045642473\n",
            "e: 234 l: 0.045513283\n",
            "e: 235 l: 0.045606658\n",
            "e: 236 l: 0.045765337\n",
            "e: 237 l: 0.045329686\n",
            "e: 238 l: 0.04529548\n",
            "e: 239 l: 0.045387544\n",
            "e: 240 l: 0.045182567\n",
            "e: 241 l: 0.045063898\n",
            "e: 242 l: 0.045041997\n",
            "e: 243 l: 0.044840787\n",
            "e: 244 l: 0.044867996\n",
            "e: 245 l: 0.04482117\n",
            "e: 246 l: 0.044851664\n",
            "e: 247 l: 0.04469247\n",
            "e: 248 l: 0.044651706\n",
            "e: 249 l: 0.044669073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H-2tnkzs0vX"
      },
      "source": [
        "# Evaluation\n",
        "Largely as before, just need to make sure batch axis exists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOUhHaE-Dqef"
      },
      "source": [
        "in_axes = [\n",
        "    [...],            # Axes for params: None, really\n",
        "    [...],            # Axes for state: Same\n",
        "    [...],            # Axes for prng: Same\n",
        "    [\"batch\", ...],   # Axes for x: batches, rest\n",
        "]\n",
        "out_axes = [\n",
        "    [\"batch\", ...],\n",
        "    [\"batch\", ...]\n",
        "]\n",
        "test_pred, _ = xmap(lambda x, y, z, w: model.apply(x, y, z, w, is_training=False), in_axes, out_axes)(params, state, None, test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gk35dT9rd3V",
        "outputId": "f9b1be29-3895-4358-95ff-5facfa9c7bf9"
      },
      "source": [
        "print(\"accuracy:\", np.mean((np.argmax(test_pred, axis = -1) - np.argmax(test_labels, axis = -1)) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.09787042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIic3e2nsrqL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}